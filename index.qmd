---
title: "index"
author: "Thomas Adams & Keegan Brown"
format: 
  html:
    self-contained: true
editor: visual
editor_options: 
  chunk_output_type: console
---

#Stretch Exercise 7

This code chunk will focus on loading the data and any libraries which will be used. The data used comes from a selection pulled from the IPUMS catalog of ACS data for the year 2021, with a focus on housing information tied to rent and geography. This will be compared to the department of Housing and Urban Development's data on Fair Market Rent calculations, which they utilize to design affordable housing programs, such as setting payments for Housing Choice Voucher programs. This data will also be use for the year 2021.

The goal of this analysis is to develop a predictive model that will identify geographic areas with rents above what is considered fair market by Housing and Urban Development, with, hopefully, further analysis within some of those individual areas to identify indicators of unfair rent pricing, e.g., dwellings with high prices given their characteristics, evidence of lack of competitive practices due to majority ownership by particular companies in an area, or a high correlation between rent/market prices and acquisition method. The target variable for this exercise will simply be gross monthly rent, a continuous variable that can be compared between HUD FMRs and the ACS data, with mind paid to the housing weights to try and tackle issues of representativeness.

```{r}
library(tidyverse)
library(tidymodels)
library(readxl)
library(ipumsr)
library(sf)
library(tigris)
library(parsnip)
library(glmnet)
us_shape <- read_ipums_sf("ipums_puma_2010")

ddi <- read_ipums_ddi("usa_00002.xml")
data <- read_ipums_micro(ddi)

fairdata <- read_xlsx("FMR2021.xlsx")

set.seed(4262023)

split <- initial_split(data, prop = .8)
ipums_train <- training(x = split)
ipums_test <- testing(x = split)

# Performing some cleaning to select only variables of interest, as well as group the PUMAs to avoid a massive dataframe

ipums_train2 <- ipums_train %>%
  filter(RENT > 0) %>%
  group_by(PUMA, STATEFIP) %>%
  summarize(mean_rent = mean(RENT))

ipums_map <- ipums_shape_inner_join(ipums_train2, us_shape, by = c("PUMA", "STATEFIP"))

ipums_map <- ipums_map %>%
  filter(State != "Alaska") %>%
  filter(State != "Hawaii")


# Examining the geographic characteristics of the data

ggplot(data = ipums_map, aes(fill = mean_rent)) +
  geom_sf() +
  scale_fill_gradient(low = "red", high = "green")
```

Pulling vacancy info. 

```{r}

##this is the hierarchical data to use that allows us to collect the vacancy proprtions
ddi_hier <- read_ipums_ddi("usa_00003.xml")
data_vac <- read_ipums_micro(ddi_hier)

#view of the vacancy piece 
data_vac%>%
  ipums_val_labels(VACANCY)

#pulling only data needed to the dataframe 
data_vac <- data_vac%>%
  select(HHWT, PUMA, VACANCY)

##changing vacancy to categorical 
data_vac <- data_vac%>%
  mutate(VACANCY = as.factor(VACANCY))

##viewing our vacancy counts, note the high count of n/a is due to hierarchical file loading format, not "real" N/A
data_vac%>%
  group_by(VACANCY)%>%
  summarise(n=n())

# we are filtering for vacancy on 1, 3 because the other vacancy types are not reflective of rental housing supply. 
data_vac <- data_vac %>%
  group_by(PUMA) %>%
  summarize(vac_ratio = sum(VACANCY %in% c(1, 3)) / n()) 


  



```



```{r}
# This is a little cluttered, so we're going to focus on Texas specifically
ipums_tex <- ipums_map %>%
  filter(State == "Texas")

ggplot(data = ipums_tex, aes(fill = mean_rent)) +
  geom_sf() +
  scale_fill_gradient(low = "red", high = "green")

# Based on this, we can see some significant price differences within the state
# Particularly, rural border areas are relatively inexpensive, while cities are much higher in value
# Not exactly surprising, but what if we narrow in on a particular area, like Austin?
```

```{r}
ipums_aus <- ipums_tex %>%
  filter(str_detect(Name,"^Austin City"))

ggplot(data = ipums_aus, aes(fill = mean_rent)) +
  geom_sf() +
  scale_fill_gradient(low = "red", high = "green")

#This gives us a nice spread! Although the scale isn't that wide, we can see across Austin rents can differ by around 600ish

#Lets compare it to our fair market data, to see about what the average housing voucher might be worth in Austin

fairdata %>%
  filter(countyname == "Austin County")
# These rents are given for different numbers of bedrooms across the county
# Based on the previous visualization, only a four-bedroom apartment approaches the average in any PUMA
# Let's rework the data to try and only get 2-bedroom apartments
```

```{r}
ipums_train_2br <- ipums_train %>%
  filter(RENT > 0) %>%
  filter(BEDROOMS == 2) %>%
  group_by(PUMA, STATEFIP) %>%
  summarize(mean_rent = mean(RENT))

ipums_map_2br <- ipums_shape_inner_join(ipums_train_2br, us_shape, by = c("PUMA", "STATEFIP"))

ipums_aus_2br <- ipums_map_2br %>%
  filter(str_detect(Name,"^Austin City"))

# We won't bother with a map for this one, we can assume the above trends hold true
# Instead, we'll just make a bar graph

ggplot(data = ipums_aus_2br) +
  geom_col(aes(x = Name,
               y = mean_rent,
               color = Name,
               fill = Name)) +
  scale_color_hue() +
  labs(title = "Average 2-Bedroom Rent in Austin PUMAs", y = "Average Rent in Dollars", x = "PUMAs") +
  theme(axis.text.x.bottom = element_blank())

```

```{r}
# Alternatively, we can examine them via table, to make it easier to compare to fair-market estimations
ipums_aus_2br %>%
  select(mean_rent)
fairdata %>%
  filter(countyname == "Austin County") %>%
  select(fmr_2)

# There is at least a 150 dollar difference between the fair market rent in Austin
# And the lowest average rent of a PUMA
# However, that *is* an average, so we can instead try and find rents that fit under the FMR
# We will also be using Gross Rent from here on out, rather than contract rent, as that's what FMR measures

ipums_train_ausFMR <- ipums_train %>%
  filter(RENTGRS > 0 & RENTGRS < 956) %>%
  filter(BEDROOMS == 2) %>%
  filter(STATEFIP == 48) %>%
  filter(PUMA >= 5302 & PUMA <= 5308)

count(ipums_train_ausFMR)

# 80 Observations! And we can use the Household weight to see how representative these are

ipums_train_ausFMR %>%
  summarize(mean(HHWT))

# The average weight of these observations comes out to be around 171, meaning
# (according to IPUMS), that the average observation in this set represents 171 or so households across the US
```

For the error metrics of this exercise, we'll be using precision and recall, as it offers a way for classification models to share some information not just about how successful they are at predicting a given outcome, but also how they're failing to predict, should that be the case.

As for the models we'll be testing, those will be fleshed out in the following code chunks.

We'll be designing models to predict classification problems. The classification we will look for is whether given information on a 2-bedroom apartment, such as location (State FIPS & PUMA), age of the building, poverty status of the inhabitant, and the inhabitants travel-time to work, and the population of a given county the PUMA falls in can provide enough statistical correlation to predict if an apartment is above or below the FMR for that county or area.

Ergo, we'll need to design the outcome variable (a_FMR for above_FMR). We'll do this by county, as that's how the FMR data is set up, and try to limit it to one state, so that we can do it by county code, as well as limit the scope of the exercise. The state, for this purpose, will be Texas, because it offers a large sample size to work with.

```{r}
ip_tex <- ipums_train %>%
  filter(RENT > 0) %>%
  filter(STATEFIP == 48) %>%
  zap_labels(COUNTYFIP) %>%
  filter(COUNTYFIP > 0) %>%
  filter(BEDROOMS == 2)

FMR_tex <- fairdata %>%
  filter(state == 48) %>%
  rename(COUNTYFIP = county)

join_tex <- left_join(x = ip_tex, y = FMR_tex, by = "COUNTYFIP")

join_tex <- join_tex %>%
  mutate(
    a_FMR = case_when(
      RENTGRS > fmr_2 ~ 1,
      RENTGRS <= fmr_2 ~ 0
    )
  )
# We'll convert our PUMAs to factors to turn them into dummies for our model
# Importantly, this will lose the geographic identification code for the PUMA
# But, we'll make it a new column so the observation can still be associated with an area
join_tex <- join_tex %>%
  mutate(PUMA_fac = as.factor(PUMA))
# Also setting our outcome to a factor, too, to use in our model
join_tex <- join_tex %>%
  mutate(a_FMR_fac = as.factor(a_FMR))
```

The models we'll consider will be as follows:

A logit-LASSO model, utilizing the lambda hyperparameter (which will be tuned for), as well as two important steps in its recipe: 1. step_dummy() to turn the PUMA codes into dummy variables to consider individually across the dataset 2. step_normalize(all_predictors()) to ensure our predictors are spread on a normal distribution for the LASSO model to interpret and subsequently reduce appropriately. We will additionally include step_corr(all_predictors()) to avoid collinearity problems that may exist in the model.

A random forest model, for which we'd also tune the mtry() hyperparameter using a regular tune_grid, from which we'd collect the best roc_auc to finalize the model with. This would use the above hyperparameters with the exception of step_normalize, which is not as important in a random forest model.

And a MARS model built for classification, which might allow for better understanding of any "hinges" in the data, without shooting for a fully logistic model like the logit-LASSO mentioned above.

The one we'll use for this exercise will be the logistic LASSO model.

Now, we design the model:

```{r}
# Start by making a penalty grid
lasso_grid <- grid_regular(penalty(), levels = 50)
# Folds for model selection
# Using our joined table up there.
# We'll clean up our testing data appropriately to apply this model, too
folds <- vfold_cv(data = join_tex, v = 10)
# Making our recipe
join_rec <- 
  recipe(a_FMR_fac ~ HHINCOME + PUMA_fac + KITCHEN + BUILTYR2 + TRANTIME + POVERTY + pop2017, data = join_tex) %>%
  step_dummy(PUMA_fac) %>%
  step_normalize(all_predictors()) %>%
  step_corr(all_predictors())

# Making our Model

join_las <- logistic_reg(
  penalty = tune(),
  mixture = 1
) %>%
  set_engine("glmnet")
# Making our workflow

las_wf <- workflow() %>%
  add_recipe(join_rec) %>%
  add_model(join_las)

# Creating a function to get the coefficients from our resamples
extractLm <- function(x) {
  x %>%
    extract_fit_engine() %>% 
    tidy()
}
lm_ctrl <- control_grid(extract = extractLm)

# Tuning our LASSO model
las_cv <- las_wf %>%
  tune_grid(
    resamples = folds,
    grid = lasso_grid,
    control = lm_ctrl
  )

# Pulling the roc_auc for this model
lasso_met<- collect_metrics(las_cv, summarize = FALSE)

lasso_met<- collect_metrics(las_cv, summarize = FALSE) %>%
  filter(.metric == "roc_auc")

# Checking the average roc_auc
mean(lasso_met$.estimate)
# What about the best lambda?
las_cv %>%
  select_best("roc_auc")
#It's fold1, model36. Let's go ahead and finalize our LASSO model
lasso_best <- las_cv %>%
  select_best("roc_auc")

lasso_final <- finalize_workflow(
  las_wf,
  parameters = lasso_best
) %>%
  fit(join_tex)

# Getting our predictions
las_tex_pred <- predict(lasso_final, join_tex, type = "class")
# calculating precision and recall for our predictions
precision_vec(join_tex$a_FMR_fac, las_tex_pred$.pred_class, "binary")
recall_vec(join_tex$a_FMR_fac, las_tex_pred$.pred_class, "binary")
```

An 82% precision rate and a 92% recall is not bad for this model. It gives fairly accurate predictions for whether a given apartment will be above FMR. The precision rate being 82% is actually somewhat interesting, as it implies that 18% of the predictions of an apartment being above FMR actually fell below the FMR threshold for their county, making them more affordable than expected. The recall is interesting, too, and was worth paying special attention to. A false negative, in this case, means that an apartment is predicted as being below FMR but the rent actually ends up being higher than the fair market value. While only 8% of the predictions fell into this category, these values indicate that there's almost 1-in-10 apartments being overcharged for, according to this model's predictive power.

To improve upon these results, the first feature engineering choice to make is obviously to include household weights to create a more representative model that could potentially be applied to a wider context. The second feature engineering choice would be to expand the dataset beyond just Texas, or beyond only 2-bedroom apartments, as these decisions simplify the model but limits its applicability to a more general context.

Modeling options that might be adjusted would include trying a different model entirely, such as the random forest model described above, or the MARS classification model to see if they offer more accurate predictions. Other options would be adjusting the recipes for the model implementation to include steps that might remove near-zero variance predictors from the model, or perhaps using a geodistance step to get an idea of how close apartments are to each other if they share a similar prediction.

That last idea is mostly conjecture, though, as I have little idea how step_geodist() works, and would have to do some investigation to see how to incorporate it.
