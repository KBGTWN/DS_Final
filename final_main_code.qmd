---
title: "final_main_code"
author: "Thomas Adams & Keegan Brown"
format: 
  html:
    self-contained: true
editor: visual
editor_options: 
  chunk_output_type: console
---

#Project Overview

#Loading Data, Libraries, and EDA

###libraries

```{r}
library(tidyverse)
library(tidymodels)
library(readxl)
library(ipumsr)
library(sf)
library(tigris)
library(parsnip)
library(glmnet)
library(patchwork)
```

###Data

```{r}
us_shape <- read_ipums_sf("ipums_puma_2010")

ddi <- read_ipums_ddi("usa_00002.xml")
data <- read_ipums_micro(ddi)

fairdata <- read_xlsx("FMR2021.xlsx")

# Performing some cleaning to select only variables of interest, as well as group the PUMAs to avoid a massive dataframe


```

###Pulling Vacancy Variable from hierarchical file

```{r}
##this is the hierarchical data to use that allows us to collect the vacancy proprtions
ddi_hier <- read_ipums_ddi("usa_00003.xml")
data_vac <- read_ipums_micro(ddi_hier)

#view of the vacancy piece 
data_vac%>%
  ipums_val_labels(VACANCY)

#pulling only data needed to the dataframe 
data_vac <- data_vac%>%
  select(HHWT, PUMA, VACANCY)

##changing vacancy to categorical 
data_vac <- data_vac%>%
  mutate(VACANCY = as.factor(VACANCY))

##viewing our vacancy counts, note the high count of n/a is due to hierarchical file loading format, not "real" N/A
data_vac%>%
  group_by(VACANCY)%>%
  summarise(n=n())

# we are filtering for vacancy on 1, 3 because the other vacancy types are not reflective of rental housing supply. 
data_vac <- data_vac %>%
  group_by(PUMA) %>%
  summarize(vac_ratio = sum(VACANCY %in% c(1, 3)) / n()) 

```

###Preprocessing/Split

```{r}
### note for TJ: this is the factorized and preprocessing i thought was needed based on the lasso you built. 

#below are all variables used in mutation and eval, pared down from all variables due to computational limits. 

data <- data%>%
  select(RENT, STATEFIP, HHWT, PUMA, BEDROOMS, RENTGRS, COUNTYFIP, TRANTIME, POVERTY, BUILTYR2, KITCHEN, HHINCOME)

## i think some of the above have to be calculated as factors too - might impact the model

data <- data%>%
  mutate(STATEFIP = as.factor(STATEFIP),
         KITCHEN = as.factor(KITCHEN),
         BUILTYR2 = as.factor(BUILTYR2),
         BEDROOMS = as.factor(BEDROOMS),
         COUNTYFIP = as.factor(COUNTYFIP))

data <- data %>%
  filter(RENT > 0)%>%
  group_by(PUMA, STATEFIP)%>%
  mutate(mean_rent = mean(RENT))%>%
  ungroup()

## Joining the vacancy variable 
data <- data %>%
  inner_join(data_vac, by = "PUMA")
  

## DC Data 
ip_DC <- data %>%
  filter(RENT > 0) %>%
  filter(STATEFIP == 11) %>%
  zap_labels(COUNTYFIP) %>%
  filter(COUNTYFIP == 1) %>%
  filter(BEDROOMS == 2)

s##Baltimore data 

ip_BLT <- data %>%
  filter(RENT > 0) %>%
  filter(STATEFIP == 24) %>%
  zap_labels(COUNTYFIP) %>%
  filter(COUNTYFIP == c(3,5,13,25,27,35,510)) %>%
  filter(BEDROOMS == 2)

```

###split

```{r}
set.seed(4262023)

##DC split 
split_dc <- initial_split(ip_DC, prop = .8)
ipums_train_dc <- training(x = split_dc)
ipums_test_dc <- testing(x = split_dc)

##Balt split 

split_blt <- initial_split(ip_BLT, prop = .8)
ipums_train_blt <- training(x = split_blt)
ipums_test_blt <- testing(x = split_blt)

## also lmiting the shapefile to just maryland and dc

us_shape <- us_shape %>%
  group_by(State) %>%
  filter(State %in% c("Maryland", "District of Columbia")) %>%
  ungroup()


```

#EDA

```{r}

## Baltimore geospatial 

blt_map <- ipums_shape_inner_join(ipums_train_blt, us_shape, by = c("PUMA", "STATEFIP"))

dc_map <- ipums_shape_inner_join(ipums_train_dc, us_shape, by = c("PUMA", "STATEFIP"))

# Examining the geographic characteristics of the data - Rent dispersion 

#Baltimore 
ggplot(data = blt_map, aes(fill = mean_rent)) +
  geom_sf() +
  scale_fill_gradient(low = "green", high = "red")+
  labs(title = "Baltimore MSA 2BR Mean Rent",
       caption = "IPUMS ACS 1-Year - 2021")+
  guides(fill=guide_legend(title="Mean Rent"))+
  theme_minimal()

#DC
ggplot(data = dc_map, aes(fill = mean_rent)) +
  geom_sf() +
  scale_fill_gradient(low = "green", high = "red")+
  labs(title = "DC MSA 2BR Mean Rent",
       caption = "IPUMS ACS 1-Year - 2021")+
  guides(fill=guide_legend(title="Mean Rent"))+
  theme_minimal()

#Vacancy rate 
ggplot(data = dc_map, aes(fill = vac_ratio)) +
  geom_sf() +
  scale_fill_gradient(low = "green", high = "red")+
  labs(title = "DC MSA 2BR Vacancy Rate",
       caption = "IPUMS ACS 1-Year - 2021")+
  guides(fill=guide_legend(title="Vacancy Rate"))


ggplot(data = blt_map, aes(fill = vac_ratio)) +
  geom_sf() +
  scale_fill_gradient(low = "green", high = "red")+
  labs(title = "Baltimore MSA 2BR Vacancy Rate",
       caption = "IPUMS ACS 1-Year - 2021")+
  guides(fill=guide_legend(title="Vacancy Rate"))


#Comparing Vacancy rate and rent prices

ggplot(data = ip_BLT, aes(x = vac_ratio, y = mean_rent)) +
    geom_point(aes(color = PUMA %in% c(801, 802, 803, 804, 805))) +
    labs(title = "Baltimore PUMA Vacancy to Mean Rent Comparison") +
    xlab("Vacancy Rate") +
    ylab("Mean Rent") +
    theme_minimal()+
    guides(color=guide_legend(title="Baltimore City"))

##DC 
ggplot(data = ip_DC, aes(x = vac_ratio, y = mean_rent)) +
    geom_point(aes(color = PUMA %in% c(104))) +
    labs(title = "DC PUMA 2BR Vacancy to Mean Rent Comparison") +
    xlab("Vacancy Rate") +
    ylab("Mean Rent") +
    theme_minimal()+
    guides(color=guide_legend(title="SE DC"))

# Alternatively, we can examine them via table, to make it easier to compare to fair-market estimations

## this needs to be fixed and maybe use rentgrs first 
ip_BLT %>%
  select(mean_rent)
fairdata %>%
  filter(countyname == "Baltimore County") %>%
  select(fmr_2)

### this needs to be fixed. 

##in Baltimore and in DC, the difference between fair market rent by PUMA ranges between [x], [y] respectively. 

```

###EDA Continued

```{r}
# Alternatively, we can examine them via table, to make it easier to compare to fair-market estimations

# There is at least a 150 dollar difference between the fair market rent in Austin
# And the lowest average rent of a PUMA
# However, that *is* an average, so we can instead try and find rents that fit under the FMR
# We will also be using Gross Rent from here on out, rather than contract rent, as that's what FMR measures

```

#Final data prep Cut?

```{r}

fmr_md <- fairdata %>%
  filter(state == 11) %>%
  rename(COUNTYFIP = county)

join_tex <- left_join(x = ip_tex, y = FMR_tex, by = "COUNTYFIP")

join_tex <- join_tex %>%
  mutate(
    a_FMR = case_when(
      RENTGRS > fmr_2 ~ 1,
      RENTGRS <= fmr_2 ~ 0
    )
  )
# We'll convert our PUMAs to factors to turn them into dummies for our model
# Importantly, this will lose the geographic identification code for the PUMA
# But, we'll make it a new column so the observation can still be associated with an area
join_tex <- join_tex %>%
  mutate(PUMA_fac = as.factor(PUMA))
# Also setting our outcome to a factor, too, to use in our model
join_tex <- join_tex %>%
  mutate(a_FMR_fac = as.factor(a_FMR))
```

#Model Spec

```{r}
# Start by making a penalty grid
lasso_grid <- grid_regular(penalty(), levels = 50)
# Folds for model selection
# Using our joined table up there.
# We'll clean up our testing data appropriately to apply this model, too
DC_folds <- vfold_cv(data = join_DC, v = 10)
# Making our recipe
DC_rec <- 
  recipe(a_FMR_fac ~ HHINCOME + PUMA_fac + KITCHEN + BUILTYR2 + TRANTIME + POVERTY + pop2017, data = join_DC) %>%
  step_dummy(PUMA_fac) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_corr(all_predictors())

# Making our Model

DC_las <- logistic_reg(
  penalty = tune(),
  mixture = 1
) %>%
  set_engine("glmnet")
# Making our workflow

DC_wf <- workflow() %>%
  add_recipe(DC_rec) %>%
  add_model(DC_las)

# Creating a function to get the coefficients from our resamples
extractLm <- function(x) {
  x %>%
    extract_fit_engine() %>% 
    tidy()
}
DC_ctrl <- control_grid(extract = extractLm)

# Tuning our LASSO model
DC_cv <- DC_wf %>%
  tune_grid(
    resamples = DC_folds,
    grid = lasso_grid,
    control = DC_ctrl
  )

# Pulling the roc_auc for this model
DC_met<- collect_metrics(DC_cv, summarize = FALSE)

DC_met<- collect_metrics(DC_cv, summarize = FALSE) %>%
  filter(.metric == "roc_auc")

# Checking the average roc_auc
mean(DC_met$.estimate)
# What about the best lambda?
DC_cv %>%
  select_best("roc_auc")
#It's fold1, model34. Let's go ahead and finalize our LASSO model
DC_best <- DC_cv %>%
  select_best("roc_auc")

DC_final <- finalize_workflow(
  DC_wf,
  parameters = DC_best
) %>%
  fit(join_DC)

# Getting our predictions
DC_pred <- predict(DC_final, join_DC, type = "class")
# calculating precision and recall for our predictions
precision_vec(join_DC$a_FMR_fac, DC_pred$.pred_class, "binary")
recall_vec(join_DC$a_FMR_fac, DC_pred$.pred_class, "binary")
```

# Running through the same steps for Baltimore

```{r}
FMR_blt <- fairdata %>%
  filter(state == 24) %>%
  rename(COUNTYFIP = county)

join_blt <- left_join(x = ip_train_blt, y = FMR_blt, by = "COUNTYFIP")

join_blt <- join_blt %>%
  mutate(
    a_FMR = case_when(
      RENTGRS > fmr_2 ~ 1,
      RENTGRS <= fmr_2 ~ 0
    )
  )

join_blt <- join_blt %>%
  mutate(PUMA_fac = as.factor(PUMA))
# Also setting our outcome to a factor, too, to use in our model
join_blt <- join_blt %>%
  mutate(a_FMR_fac = as.factor(a_FMR))
# Uncounting the HHWT to get a representative sample
join_blt <- join_blt %>%
  uncount(HHWT)
```

```{r}
blt_folds <- vfold_cv(data = join_blt, v = 10)
# Making our recipe
blt_rec <- 
  recipe(a_FMR_fac ~ HHINCOME + PUMA_fac + KITCHEN + BUILTYR2 + TRANTIME + POVERTY + pop2017, data = join_blt) %>%
  step_dummy(PUMA_fac) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors()) %>%
  step_corr(all_predictors())

# Making our Model

blt_las <- logistic_reg(
  penalty = tune(),
  mixture = 1
) %>%
  set_engine("glmnet")
# Making our workflow

blt_wf <- workflow() %>%
  add_recipe(blt_rec) %>%
  add_model(blt_las)

blt_ctrl <- control_grid(extract = extractLm)

# Tuning our LASSO model
blt_cv <- blt_wf %>%
  tune_grid(
    resamples = blt_folds,
    grid = lasso_grid,
    control = blt_ctrl
  )

# Pulling the roc_auc for this model
blt_met<- collect_metrics(blt_cv, summarize = FALSE)

blt_met<- collect_metrics(blt_cv, summarize = FALSE) %>%
  filter(.metric == "roc_auc")

# Checking the average roc_auc
mean(blt_met$.estimate)
# What about the best lambda?
blt_cv %>%
  select_best("roc_auc")
#It's fold1, model34. Let's go ahead and finalize our LASSO model
blt_best <- blt_cv %>%
  select_best("roc_auc")

blt_final <- finalize_workflow(
  blt_wf,
  parameters = blt_best
) %>%
  fit(join_blt)

# Getting our predictions
blt_pred <- predict(blt_final, join_blt, type = "class")
# calculating precision and recall for our predictions
precision_vec(join_blt$a_FMR_fac, blt_pred$.pred_class, "binary")
recall_vec(join_blt$a_FMR_fac, blt_pred$.pred_class, "binary")
```

Both of these models offer satisfying precision and recall rates, at roughly low-to-mid 80%s in both categories. We will now try them with the testing data. In order to do this, the testing data has to be cleaned in roughly the same way.

```{r}
join_blt_test <- left_join(x = ip_test_blt, y = FMR_blt, by = "COUNTYFIP")

join_blt_test <- join_blt_test %>%
  mutate(
    a_FMR = case_when(
      RENTGRS > fmr_2 ~ 1,
      RENTGRS <= fmr_2 ~ 0
    )
  )

join_blt_test <- join_blt_test %>%
  mutate(PUMA_fac = as.factor(PUMA))
# Also setting our outcome to a factor, too, to use in our model
join_blt_test <- join_blt_test %>%
  mutate(a_FMR_fac = as.factor(a_FMR))
# Uncounting the HHWT to get a representative sample
join_blt_test <- join_blt_test %>%
  uncount(HHWT)



join_DC_test <- left_join(x = ip_test_DC, y = FMR_DC, by = "COUNTYFIP")

join_DC_test <- join_DC_test %>%
  mutate(
    a_FMR = case_when(
      RENTGRS > fmr_2 ~ 1,
      RENTGRS <= fmr_2 ~ 0
    )
  )
# We'll convert our PUMAs to factors to turn them into dummies for our model
# Importantly, this will lose the geographic identification code for the PUMA
# But, we'll make it a new column so the observation can still be associated with an area
join_DC_test <- join_DC_test %>%
  mutate(PUMA_fac = as.factor(PUMA))
# Also setting our outcome to a factor, too, to use in our model
join_DC_test <- join_DC_test %>%
  mutate(a_FMR_fac = as.factor(a_FMR))
# Uncounting the HHWT to get a representative sample
join_DC_test <- join_DC_test %>%
  uncount(HHWT)
```

Now we'll get predictions from our finalized models for the testing data and see what happens.

```{r}
DC_pred_test <- predict(DC_final, join_DC_test, type = "class")

precision_vec(join_DC_test$a_FMR_fac, DC_pred_test$.pred_class, "binary")
recall_vec(join_DC_test$a_FMR_fac, DC_pred_test$.pred_class, "binary")

blt_pred_test <- predict(blt_final, join_blt_test, type = "class")

precision_vec(join_blt_test$a_FMR_fac, blt_pred_test$.pred_class, "binary")
recall_vec(join_blt_test$a_FMR_fac, blt_pred_test$.pred_class, "binary")
```

Precision and recall still hover in that range for DC, but Baltimore has a much lower precision, as 69%, and a much higher recall, at 89%, meaning the model is slightly less accurate for that area.