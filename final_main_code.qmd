---
title: "final_main_code"
author: "Thomas Adams & Keegan Brown"
format: 
  html:
    self-contained: true
editor: visual
editor_options: 
  chunk_output_type: console
---

#Project Overview

#Loading Data, Libraries, and EDA

###libraries

```{r}
library(tidyverse)
library(tidymodels)
library(readxl)
library(ipumsr)
library(sf)
library(tigris)
library(parsnip)
library(glmnet)
library(patchwork)
library(ranger)
```

###Data

```{r}
#Shapefile
us_shape <- read_ipums_sf("ipums_puma_2010")

#IPUMS 2021 1year ACS extract/ddi
ddi <- read_ipums_ddi("usa_00002.xml")
data <- read_ipums_micro(ddi)

#HUD FMH data - 
fairdata <- read_xlsx("FMR2021.xlsx")

```

###Pulling Vacancy Variable from hierarchical file

```{r}
##this is the hierarchical data to use that allows us to collect the vacancy proprtions
ddi_hier <- read_ipums_ddi("usa_00003.xml")
data_vac <- read_ipums_micro(ddi_hier)

#view of the vacancy piece 
data_vac%>%
  ipums_val_labels(VACANCY)

#pulling only data needed to the dataframe 
data_vac <- data_vac%>%
  select(HHWT, PUMA, VACANCY)

##changing vacancy to categorical 
data_vac <- data_vac%>%
  mutate(VACANCY = as.factor(VACANCY))

##viewing our vacancy counts, note the high count of n/a is due to hierarchical file loading format, not "real" N/A
data_vac%>%
  group_by(VACANCY)%>%
  summarise(n=n())

# we are filtering for vacancy on 1, 3 because the other vacancy types are not reflective of rental housing supply. 
data_vac <- data_vac %>%
  group_by(PUMA) %>%
  summarize(vac_ratio = sum(VACANCY %in% c(1, 3)) / n()) 

```

###Preprocessing/Split

```{r}

#below are all variables used in mutation and eval, pared down from all variables due to computational limits. 

data <- data%>%
  select(RENT, STATEFIP, HHWT, PUMA, BEDROOMS, RENTGRS, COUNTYFIP, TRANTIME, POVERTY, BUILTYR2, KITCHEN, HHINCOME, DENSITY)

##Mutating required variables into factor variables for EDA and model implementation

data <- data%>%
  mutate(STATEFIP = as.factor(STATEFIP),
         KITCHEN = as.factor(KITCHEN),
         BUILTYR2 = as.factor(BUILTYR2),
         BEDROOMS = as.factor(BEDROOMS),
         COUNTYFIP = as.factor(COUNTYFIP))

data <- data %>%
  group_by(PUMA, STATEFIP)%>%
  mutate(mean_rent = mean(RENT))%>%
  ungroup()

data <- data%>%
  group_by(STATEFIP)%>%
  filter(STATEFIP == 11 | STATEFIP == 24)%>%
  ungroup()

##proof of STATEFIP filter 
data%>%
  group_by(STATEFIP)%>%
  summarise(n=n())

## Joining the vacancy variable 
data <- data %>%
  inner_join(data_vac, by = "PUMA")

##filter for DC/ Baltimore to be included only under the proper conditions
data <- data %>%
  group_by(STATEFIP) %>%
  filter(STATEFIP == 11 & COUNTYFIP == 1 | 
         STATEFIP == 24 & COUNTYFIP %in% c(3, 5, 13, 25, 27, 35, 510)) %>%
  ungroup()

## final filters to align with the rest of the dataset 
data <- data%>%
  filter(RENT > 0)%>%
  filter(BEDROOMS == 2)

## DC Data only
ip_DC <- data %>%
  filter(STATEFIP == 11) %>%
  zap_labels(COUNTYFIP) %>%
  filter(COUNTYFIP == 1) %>%
  filter(BEDROOMS == 2)

##Baltimore data only 
ip_BLT <- data %>%
  filter(STATEFIP == 24) %>%
  zap_labels(COUNTYFIP) %>%
  filter(COUNTYFIP == c(3,5,13,25,27,35,510)) %>%
  filter(BEDROOMS == 2)

```

###split - 3 ways

```{r}
set.seed(4262023)

split <- initial_split(data, prop = .8)
ipums_train_tot <- training (x = split)
ipums_test_tot <- testing(x=split)

##DC split 
split_dc <- initial_split(ip_DC, prop = .8)
ipums_train_dc <- training(x = split_dc)
ipums_test_dc <- testing(x = split_dc)

##Balt split 

split_blt <- initial_split(ip_BLT, prop = .8)
ipums_train_blt <- training(x = split_blt)
ipums_test_blt <- testing(x = split_blt)


## also lmiting the shapefile to just maryland and dc

us_shape <- us_shape %>%
  group_by(State) %>%
  filter(State %in% c("Maryland", "District of Columbia")) %>%
  ungroup()

```

#EDA

```{r}

## Baltimore geospatial 

blt_map <- ipums_shape_inner_join(ipums_train_blt, us_shape, by = c("PUMA", "STATEFIP"))

dc_map <- ipums_shape_inner_join(ipums_train_dc, us_shape, by = c("PUMA", "STATEFIP"))

# Examining the geographic characteristics of the data - Rent dispersion 

#Baltimore 
ggplot(data = blt_map, aes(fill = mean_rent)) +
  geom_sf() +
  scale_fill_gradient(low = "green", high = "red")+
  labs(title = "Baltimore MSA 2BR Mean Rent",
       caption = "IPUMS ACS 1-Year - 2021")+
  guides(fill=guide_legend(title="Mean Rent"))+
  theme_minimal()

#DC
ggplot(data = dc_map, aes(fill = mean_rent)) +
  geom_sf() +
  scale_fill_gradient(low = "green", high = "red")+
  labs(title = "DC MSA 2BR Mean Rent",
       caption = "IPUMS ACS 1-Year - 2021")+
  guides(fill=guide_legend(title="Mean Rent"))+
  theme_minimal()

#Vacancy rate 
ggplot(data = dc_map, aes(fill = vac_ratio)) +
  geom_sf() +
  scale_fill_gradient(low = "green", high = "red")+
  labs(title = "DC MSA 2BR Vacancy Rate",
       caption = "IPUMS ACS 1-Year - 2021")+
  guides(fill=guide_legend(title="Vacancy Rate"))


ggplot(data = blt_map, aes(fill = vac_ratio)) +
  geom_sf() +
  scale_fill_gradient(low = "green", high = "red")+
  labs(title = "Baltimore MSA 2BR Vacancy Rate",
       caption = "IPUMS ACS 1-Year - 2021")+
  guides(fill=guide_legend(title="Vacancy Rate"))


#Comparing Vacancy rate and rent prices

ggplot(data = ip_BLT, aes(x = vac_ratio, y = mean_rent)) +
    geom_point(aes(color = PUMA %in% c(801, 802, 803, 804, 805))) +
    labs(title = "Baltimore PUMA Vacancy to Mean Rent Comparison") +
    xlab("Vacancy Rate") +
    ylab("Mean Rent") +
    theme_minimal()+
    guides(color=guide_legend(title="Baltimore City"))

##DC 
ggplot(data = ip_DC, aes(x = vac_ratio, y = mean_rent)) +
    geom_point(aes(color = PUMA %in% c(104))) +
    labs(title = "DC PUMA 2BR Vacancy to Mean Rent Comparison") +
    xlab("Vacancy Rate") +
    ylab("Mean Rent") +
    theme_minimal()+
    guides(color=guide_legend(title="SE DC"))

# Alternatively, we can examine them via table, to make it easier to compare to fair-market estimations


## this needs to be fixed and maybe use rentgrs first 
ip_BLT %>%
  select(mean_rent)
fairdata %>%
  filter(countyname == "Baltimore County") %>%
  select(fmr_2)

### this needs to be fixed. 

##in Baltimore and in DC, the difference between fair market rent by PUMA ranges between [x], [y] respectively.  - maybe one where size = density? 

state_labels <- c("11" = "DC", "24" = "Maryland")

ggplot(data = ipums_train_tot,
       aes(x = vac_ratio, y = mean_rent, color = STATEFIP, size = DENSITY))+
  geom_point()+
  labs(title = "DC and Baltimore Mean Rent vs. Vacancy",
       caption = "Source: IPUMS ACS 1-Year 2021",
       color = "State",
       size = "Density (Square Mile)")+
  scale_color_manual(values = c("11" = "brown1", "24" = "darkturquoise"),
                     labels = state_labels)+
  xlab("Vacancy Rate")+
  ylab("Mean Rent")+
  theme_classic()

```

#Final Prep of data

## Overall

```{r}
## filtering FMR data to only the dc and boston areas of consideration
fmr_tot <- fairdata%>%
    rename(COUNTYFIP = county)%>%
    filter(state == 11 & COUNTYFIP == 1 | 
         state == 24 & COUNTYFIP %in% c(3, 5, 13, 25, 27, 35, 510))

## ensuring countyfip is a factor so it can connect with the ipums_train data
fmr_tot <- fmr_tot%>%
  mutate(COUNTYFIP = as.factor(COUNTYFIP))

## join
join_tot <- left_join(x= ipums_train_tot, y=fmr_tot, by = "COUNTYFIP")


#fmr eval class 
join_tot <- join_tot %>%
  mutate(
    a_FMR = case_when(
      RENTGRS > fmr_2 ~ 1,
      RENTGRS <= fmr_2 ~ 0
    )
  )

join_tot <- join_tot %>%
  mutate(PUMA_fac = as.factor(PUMA))
# Also setting our outcome to a factor, too, to use in our model
join_tot <- join_tot %>%
  mutate(a_FMR_fac = as.factor(a_FMR))
#Uncounting to take sample weight into account
join_tot <- join_tot %>%
  uncount(HHWT)
```

##DC

```{r}

FMR_DC <- fairdata %>%
  filter(state == 11) %>%
  rename(COUNTYFIP = county)

FMR_DC <- FMR_DC%>%
  mutate(COUNTYFIP = as.factor(COUNTYFIP))
  

join_dc <- left_join(x = ipums_train_dc, y = FMR_DC, by = "COUNTYFIP")

join_dc <- join_dc %>%
  mutate(
    a_FMR = case_when(
      RENTGRS > fmr_2 ~ 1,
      RENTGRS <= fmr_2 ~ 0
    )
  )
# We'll convert our PUMAs to factors to turn them into dummies for our model
# Importantly, this will lose the geographic identification code for the PUMA
# But, we'll make it a new column so the observation can still be associated with an area
join_dc <- join_dc %>%
  mutate(PUMA_fac = as.factor(PUMA))
# Also setting our outcome to a factor, too, to use in our model
join_dc <- join_dc %>%
  mutate(a_FMR_fac = as.factor(a_FMR))

#Uncounting
join_dc <- join_dc %>%
  uncount(HHWT)
```

##Baltimore

```{r}
FMR_blt <- fairdata %>%
  filter(state == 24) %>%
  rename(COUNTYFIP = county)%>%
  mutate(COUNTYFIP = as.factor(COUNTYFIP))

join_blt <- left_join(x = ipums_train_blt, y = FMR_blt, by = "COUNTYFIP")

join_blt <- join_blt %>%
  mutate(
    a_FMR = case_when(
      RENTGRS > fmr_2 ~ 1,
      RENTGRS <= fmr_2 ~ 0
    )
  )

join_blt <- join_blt %>%
  mutate(PUMA_fac = as.factor(PUMA))
# Also setting our outcome to a factor, too, to use in our model
join_blt <- join_blt %>%
  mutate(a_FMR_fac = as.factor(a_FMR))

# Uncounting here to take sample weights into account
join_blt <- join_blt %>%
  uncount(HHWT)

```

#Model Specs - Lasso

##Overall

```{r}

# Start by making a penalty grid
lasso_grid <- grid_regular(penalty(), levels = 50)
# Folds for model selection
# Using our joined table up there.
# We'll clean up our testing data appropriately to apply this model, too
tot_folds <- vfold_cv(data = join_tot, v = 10)
# Making our recipe
tot_rec <- 
  recipe(a_FMR_fac ~ HHINCOME + STATEFIP + PUMA_fac + KITCHEN + BUILTYR2 + TRANTIME + POVERTY + vac_ratio + pop2017, data = join_tot) %>%
  step_dummy(all_factor_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric()) %>%
  step_corr(all_numeric())

# Making our Model
tot_las <- logistic_reg(
  penalty = tune(),
  mixture = 1
) %>%
  set_engine("glmnet")
# Making our workflow

tot_wf <- workflow() %>%
  add_recipe(tot_rec) %>%
  add_model(tot_las)
# Creating a function to get the coefficients from our resamples
extractLm <- function(x) {
  x %>%
    extract_fit_engine() %>% 
    tidy()
}
tot_ctrl <- control_grid(extract = extractLm)

# Tuning our LASSO model
tot_cv <- tot_wf %>%
  tune_grid(
    resamples = tot_folds,
    grid = lasso_grid,
    control = tot_ctrl
  )

# Pulling the roc_auc for this model
tot_met<- collect_metrics(tot_cv, summarize = FALSE)

tot_met<- collect_metrics(tot_cv, summarize = FALSE) %>%
  filter(.metric == "roc_auc")

# Checking the average roc_auc
mean(tot_met$.estimate)
# What about the best lambda?
tot_cv %>%
  select_best("roc_auc")

tot_best <- tot_cv %>%
  select_best("roc_auc")

tot_final <- finalize_workflow(
  tot_wf,
  parameters = tot_best
) %>%
  fit(join_tot)

# Getting our predictions
tot_pred <- predict(tot_final, join_tot, type = "class")
# calculating precision and recall for our predictions
precision_vec(join_tot$a_FMR_fac, tot_pred$.pred_class, "binary")
recall_vec(join_tot$a_FMR_fac,tot_pred$.pred_class, "binary")
```

##DC

```{r}
# Start by making a penalty grid
lasso_grid <- grid_regular(penalty(), levels = 50)
# Folds for model selection
# Using our joined table up there.
# We'll clean up our testing data appropriately to apply this model, too
DC_folds <- vfold_cv(data = join_dc, v = 10)
# Making our recipe
DC_rec <- 
  recipe(a_FMR_fac ~ HHINCOME + PUMA_fac + KITCHEN + BUILTYR2 + TRANTIME + POVERTY + vac_ratio + pop2017, data = join_tot) %>%
  step_dummy(all_factor_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric()) %>%
  step_corr(all_numeric())

# Making our Model

DC_las <- logistic_reg(
  penalty = tune(),
  mixture = 1
) %>%
  set_engine("glmnet")
# Making our workflow

DC_wf <- workflow() %>%
  add_recipe(DC_rec) %>%
  add_model(DC_las)

# Creating a function to get the coefficients from our resamples
extractLm <- function(x) {
  x %>%
    extract_fit_engine() %>% 
    tidy()
}
DC_ctrl <- control_grid(extract = extractLm)

# Tuning our LASSO model
DC_cv <- DC_wf %>%
  tune_grid(
    resamples = DC_folds,
    grid = lasso_grid,
    control = DC_ctrl
  )

# Pulling the roc_auc for this model
DC_met<- collect_metrics(DC_cv, summarize = FALSE)

DC_met<- collect_metrics(DC_cv, summarize = FALSE) %>%
  filter(.metric == "roc_auc")

# Checking the average roc_auc
mean(DC_met$.estimate)
# What about the best lambda?
DC_cv %>%
  select_best("roc_auc")
#It's fold1, model34. Let's go ahead and finalize our LASSO model
DC_best <- DC_cv %>%
  select_best("roc_auc")

DC_final <- finalize_workflow(
  DC_wf,
  parameters = DC_best
) %>%
  fit(join_dc)

# Getting our predictions
DC_pred <- predict(DC_final, join_dc, type = "class")
# calculating precision and recall for our predictions
precision_vec(join_dc$a_FMR_fac, DC_pred$.pred_class, "binary")
recall_vec(join_dc$a_FMR_fac, DC_pred$.pred_class, "binary")
```

##Baltimore

```{r}
blt_folds <- vfold_cv(data = join_blt, v = 10)
# Making our recipe
blt_rec <- 
  recipe(a_FMR_fac ~ HHINCOME + PUMA_fac + KITCHEN + BUILTYR2 + TRANTIME + POVERTY + vac_ratio + pop2017, data = join_tot) %>%
  step_dummy(all_factor_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric()) %>%
  step_corr(all_numeric())

# Making our Model

blt_las <- logistic_reg(
  penalty = tune(),
  mixture = 1
) %>%
  set_engine("glmnet")
# Making our workflow

blt_wf <- workflow() %>%
  add_recipe(blt_rec) %>%
  add_model(blt_las)

blt_ctrl <- control_grid(extract = extractLm)

# Tuning our LASSO model
blt_cv <- blt_wf %>%
  tune_grid(
    resamples = blt_folds,
    grid = lasso_grid,
    control = blt_ctrl
  )

# Pulling the roc_auc for this model
blt_met<- collect_metrics(blt_cv, summarize = FALSE)

blt_met<- collect_metrics(blt_cv, summarize = FALSE) %>%
  filter(.metric == "roc_auc")

# Checking the average roc_auc
mean(blt_met$.estimate)
# What about the best lambda?
blt_cv %>%
  select_best("roc_auc")
#It's fold1, model34. Let's go ahead and finalize our LASSO model
blt_best <- blt_cv %>%
  select_best("roc_auc")

blt_final <- finalize_workflow(
  blt_wf,
  parameters = blt_best
) %>%
  fit(join_blt)

# Getting our predictions
blt_pred <- predict(blt_final, join_blt, type = "class")
# calculating precision and recall for our predictions
precision_vec(join_blt$a_FMR_fac, blt_pred$.pred_class, "binary")
recall_vec(join_blt$a_FMR_fac, blt_pred$.pred_class, "binary")
```

Both of these models offer satisfying precision and recall rates, at roughly low-to-mid 80%s in both categories. We will now try them with the testing data. In order to do this, the testing data has to be cleaned in roughly the same way.

##cleaning for test comparison

```{r}
join_tot_test <- left_join(x = ipums_test_tot, y = fmr_tot, by = "COUNTYFIP")

join_tot_test <- join_tot_test %>%
  mutate(
    a_FMR = case_when(
      RENTGRS > fmr_2 ~ 1,
      RENTGRS <= fmr_2 ~ 0
    )
  )

join_tot_test <- join_tot_test %>%
  mutate(PUMA_fac = as.factor(PUMA))

join_tot_test <- join_tot_test %>%
  mutate(a_FMR_fac = as.factor(a_FMR))

join_tot_test <- join_tot_test %>%
  uncount(HHWT)

join_blt_test <- left_join(x = ipums_test_blt, y = FMR_blt, by = "COUNTYFIP")

join_blt_test <- join_blt_test %>%
  mutate(
    a_FMR = case_when(
      RENTGRS > fmr_2 ~ 1,
      RENTGRS <= fmr_2 ~ 0
    )
  )

join_blt_test <- join_blt_test %>%
  mutate(PUMA_fac = as.factor(PUMA))
# Also setting our outcome to a factor, too, to use in our model
join_blt_test <- join_blt_test %>%
  mutate(a_FMR_fac = as.factor(a_FMR))
# Uncounting the HHWT to get a representative sample
join_blt_test <- join_blt_test %>%
  uncount(HHWT)



join_dc_test <- left_join(x = ipums_test_dc, y = FMR_DC, by = "COUNTYFIP")

join_dc_test <- join_dc_test %>%
  mutate(
    a_FMR = case_when(
      RENTGRS > fmr_2 ~ 1,
      RENTGRS <= fmr_2 ~ 0
    )
  )
# We'll convert our PUMAs to factors to turn them into dummies for our model
# Importantly, this will lose the geographic identification code for the PUMA
# But, we'll make it a new column so the observation can still be associated with an area
join_dc_test <- join_dc_test %>%
  mutate(PUMA_fac = as.factor(PUMA))
# Also setting our outcome to a factor, too, to use in our model
join_dc_test <- join_dc_test %>%
  mutate(a_FMR_fac = as.factor(a_FMR))
# Uncounting the HHWT to get a representative sample
join_dc_test <- join_dc_test %>%
  uncount(HHWT)
```

### Test and eval

Now we'll get predictions from our finalized models for the testing data and see what happens.

```{r}
tot_pred_test <- predict(tot_final, join_tot_test, type = "class")

precision_vec(join_tot_test$a_FMR_fac, tot_pred_test$.pred_class, "binary")
recall_vec(join_tot_test$a_FMR_fac, tot_pred_test$.pred_class, "binary")

DC_pred_test <- predict(DC_final, join_dc_test, type = "class")

precision_vec(join_dc_test$a_FMR_fac, DC_pred_test$.pred_class, "binary")
recall_vec(join_dc_test$a_FMR_fac, DC_pred_test$.pred_class, "binary")

blt_pred_test <- predict(blt_final, join_blt_test, type = "class")

precision_vec(join_blt_test$a_FMR_fac, blt_pred_test$.pred_class, "binary")
recall_vec(join_blt_test$a_FMR_fac, blt_pred_test$.pred_class, "binary")
```

Precision and recall still hover in that range for DC, but Baltimore has a much lower precision, as 69%, and a much higher recall, at 89%, meaning the model is slightly less accurate for that area.

Our model with the combined dataset has even better results, with a precision rate of 87.6% and a recall of 83.3%. 

#Model Spec- RF

From this poiunt on, there is an example of running the rf model for the sake of the stretch ex - per discussion with Prof. Williams. Above is model of choice. 

```{r}

##Setting up cv 
tot_folds <- vfold_cv(data = join_tot, v = 10)

## setting up recipe 
rf_rec <-
  recipe(a_FMR_fac ~ HHINCOME + PUMA_fac + KITCHEN + BUILTYR2 + TRANTIME + POVERTY + pop2017 + vac_ratio, data = join_tot)%>%
  step_dummy(all_factor_predictors(), -all_outcomes())%>%
  step_normalize(all_double_predictors())%>%
  step_normalize(vac_ratio)
  
prep(rf_rec)

## setting up model spec
## this section is for sample only, obviously 10 trees is pretty low. 

exp_spec <- rand_forest(
  min_n = tune(), 
  trees = 10,
  mtry = tune()
) %>%
  set_engine("ranger")%>%
 set_mode("classification")

## setting up workflow 
exp_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(exp_spec)

## implementing WF
exp_res <- tune_grid(
  exp_wf,
  resamples = tot_folds,
  grid = 20
)

exp_res

results <- exp_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc")


plot_rf_tune1 <- ggplot(data = results, aes(x = mtry, y = mean)) +
  geom_point(show.legend = FALSE) +
  labs(x = NULL, y = "AUC")

plot_rf_tune2 <- ggplot(data = results, aes(x = min_n, y = mean)) +
  geom_point(show.legend = FALSE) +
  labs(x =NULL, y = "AUC")

plot_rf_tune1 + plot_rf_tune2

```

We can see from the above graphic and model specific specification that our best AUC's are with predictors that are between 5-10, whereas our min_n range should be between 5-15. Below we tune these paramters to optimize our AUC.

##Hyperparameter tuned

```{r}

##tuned hyperparameter grid - chose 3 for limited 
new_grid <- grid_regular(
  mtry(range = c(5, 10)),
  min_n(range = c(5, 15)),
  levels = 3
)

## implementing wf for tuned product
tune_rf <- tune_grid(
  exp_wf,
  resamples = tot_folds,
  grid = new_grid
)

### putting metrics into separate df
tune_results <- tune_rf %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc")

##plotting vor viz
plot_rf_tuned1 <- ggplot(data = tune_results, aes(x = mtry, y = mean)) +
  geom_point(show.legend = FALSE) +
  labs(x = NULL, y = "AUC")

plot_rf_tuned2 <- ggplot(data = tune_results, aes(x = min_n, y = mean)) +
  geom_point(show.legend = FALSE) +
  labs(x =NULL, y = "AUC")

plot_rf_tuned1
plot_rf_tuned2

```

## Finalized model and eval

```{r}

## selecting the best combination of min_n, parameters, paramater counts to optimize AUC
optimized <- select_best(tune_rf, "roc_auc")

## setting up the model spec
optimal_rf <- 
  finalize_workflow(
    exp_wf, 
    optimized
  ) %>%
  fit(join_tot)
#viewing results 
optimal_rf

##It appears that our optimal model with this seed is 10 candidate variables and 5 minimum observations.

predictions <- predict(object = optimal_rf, new_data = join_tot_test, type = "class" )


```


